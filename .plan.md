# OpenClaw Voice Layer - Implementation Plan

## Overview
A local voice interface for OpenClaw that provides always-on wake word detection, speech-to-text, and text-to-speech with a minimal JARVIS-style overlay UI.

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     macOS Overlay (SwiftUI)                 │
│  - Floating waveform visualization                          │
│  - Status indicators (listening/processing/speaking)        │
│  - Minimal, always-on-top window                            │
└─────────────────────────────────────────────────────────────┘
                              │
                              │ Unix Socket / stdin-stdout
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    Voice Engine (Python)                     │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │ OpenWakeWord│→ │ mlx-whisper │→ │ OpenClaw WS Client  │  │
│  │ (wake word) │  │   (STT)     │  │ (send/receive msgs) │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
│                                              │               │
│                                              ▼               │
│                                    ┌─────────────────────┐  │
│                                    │ Piper TTS (speak)   │  │
│                                    └─────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │
                              │ WebSocket (ws://127.0.0.1:18789)
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                OpenClaw Gateway (existing)                   │
│  - sessions_send: send user message                         │
│  - streaming response events                                │
└─────────────────────────────────────────────────────────────┘
```

## Components

### 1. Voice Engine (Python) - `voice/`

**Dependencies:**
- `openwakeword` - Wake word detection (80ms frames, 0-1 confidence)
- `mlx-whisper` or `lightning-whisper-mlx` - Speech-to-text on Apple Silicon
- `piper-tts` - Neural text-to-speech
- `websockets` - Connect to OpenClaw Gateway
- `pyaudio` or `sounddevice` - Audio capture/playback
- `numpy` - Audio buffer handling

**Files:**
```
voice/
├── src/
│   ├── __init__.py
│   ├── main.py              # Entry point, orchestrates components
│   ├── audio.py             # Audio capture (microphone) and playback
│   ├── wake_word.py         # OpenWakeWord wrapper
│   ├── stt.py               # mlx-whisper speech-to-text
│   ├── tts.py               # Piper text-to-speech
│   ├── gateway_client.py    # OpenClaw WebSocket client
│   └── config.py            # Configuration (wake word, model paths, etc.)
├── requirements.txt
└── pyproject.toml
```

**Flow:**
1. Audio stream continuously feeds OpenWakeWord (80ms chunks)
2. On wake word detection (score > threshold), switch to recording mode
3. Record until silence detected (VAD), then transcribe with mlx-whisper
4. Send transcription to OpenClaw via WebSocket (`sessions_send`)
5. Receive streamed response, speak via Piper TTS
6. Return to wake word listening mode

### 2. macOS Overlay UI (SwiftUI) - `ui/JarvisOverlay/`

**Features:**
- Floating, borderless, always-on-top window
- Waveform visualization of audio input
- Status states: idle, listening, processing, speaking
- Click-through when idle (doesn't steal focus)
- Draggable to reposition

**Files:**
```
ui/JarvisOverlay/
├── JarvisOverlay/
│   ├── JarvisOverlayApp.swift
│   ├── ContentView.swift
│   ├── WaveformView.swift
│   ├── VoiceEngine.swift      # Communicates with Python voice engine
│   └── AudioVisualization.swift
├── JarvisOverlay.xcodeproj/
└── Package.swift
```

**Communication with Voice Engine:**
- Option A: Launch Python as subprocess, communicate via stdin/stdout JSON
- Option B: Unix domain socket for bidirectional communication
- Events: `wake_detected`, `listening`, `transcription`, `response_start`, `response_chunk`, `response_end`, `speaking`, `idle`

### 3. Model Downloads - `scripts/`

```
scripts/
├── download_models.sh       # Downloads all required models
└── setup.sh                 # Full setup script
```

**Models to download:**
- Wake word: `hey_jarvis.onnx` or custom model from OpenWakeWord
- Whisper: `mlx-community/whisper-small` (or medium/large for accuracy)
- Piper: `en_US-lessac-medium.onnx` (or other voice)

## Implementation Tasks

### Phase 1: Voice Engine Core
1. [ ] Set up Python project with pyproject.toml
2. [ ] Implement audio capture with sounddevice
3. [ ] Integrate OpenWakeWord for wake word detection
4. [ ] Implement silence detection (VAD) for end-of-utterance
5. [ ] Integrate mlx-whisper for transcription
6. [ ] Integrate Piper TTS for speech output
7. [ ] Create OpenClaw Gateway WebSocket client
8. [ ] Wire up the full voice loop

### Phase 2: macOS Overlay UI
9. [ ] Create SwiftUI app with floating window
10. [ ] Implement waveform visualization
11. [ ] Add status indicator (idle/listening/processing/speaking)
12. [ ] Connect to Python voice engine via subprocess/socket
13. [ ] Handle drag-to-reposition

### Phase 3: Integration & Polish
14. [ ] Model download scripts
15. [ ] Full setup script (venv, deps, models)
16. [ ] Configuration file for customization
17. [ ] README with usage instructions

## OpenClaw Gateway Protocol (Research Notes)

Based on research, the Gateway uses:
- WebSocket at `ws://127.0.0.1:18789`
- Methods likely include:
  - `sessions_send` - Send a user message
  - `sessions_list` - List sessions
  - `sessions_history` - Get session history
  - Streaming response events

**Note:** We may need to inspect OpenClaw's source or an existing client (like WebChat) to understand the exact message format. The protocol appears to be JSON-based over WebSocket.

## Configuration

```yaml
# config.yaml
wake_word:
  model: "hey_jarvis"  # or path to custom model
  threshold: 0.5

stt:
  model: "mlx-community/whisper-small"
  language: "en"

tts:
  model: "en_US-lessac-medium"
  rate: 1.0

gateway:
  url: "ws://127.0.0.1:18789"
  session_id: null  # auto-create or specify

audio:
  sample_rate: 16000
  chunk_size: 1280  # 80ms at 16kHz
  silence_threshold: 0.01
  silence_duration: 1.0  # seconds
```

## Risks & Mitigations

1. **Gateway protocol unknown** - May need to reverse-engineer from OpenClaw source or existing clients. Mitigation: Start with a simple test client to probe the protocol.

2. **Piper macOS ARM64 support** - Piper claims ARM64 support but may have issues. Mitigation: Fall back to `say` command or macOS AVSpeechSynthesizer if needed.

3. **Audio latency** - Multiple processing stages could add latency. Mitigation: Use streaming where possible, optimize buffer sizes.

4. **Model memory** - Whisper + wake word + TTS models in memory. Mitigation: Use smaller models, lazy-load TTS.
