# OpenClaw Voice Layer - Implementation Plan

## Overview
A local voice interface for OpenClaw that provides always-on wake word detection, speech-to-text, and text-to-speech with a minimal JARVIS-style overlay UI.

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     macOS Overlay (SwiftUI)                 │
│  - Floating waveform visualization                          │
│  - Status indicators (listening/processing/speaking)        │
│  - Minimal, always-on-top window                            │
└─────────────────────────────────────────────────────────────┘
                              │
                              │ Unix Socket (/tmp/openclaw_voice.sock)
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    Voice Engine (Python)                     │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │ OpenWakeWord│→ │ mlx-whisper │→ │ OpenClaw WS Client  │  │
│  │ (wake word) │  │   (STT)     │  │ (send/receive msgs) │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
│                                              │               │
│                                              ▼               │
│                                    ┌─────────────────────┐  │
│                                    │ Piper TTS (speak)   │  │
│                                    └─────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │
                              │ WebSocket (ws://127.0.0.1:18789)
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                OpenClaw Gateway (existing)                   │
│  - sessions_send: send user message                         │
│  - streaming response events                                │
└─────────────────────────────────────────────────────────────┘
```

## State Machine

The voice engine uses an explicit state machine to prevent race conditions and simplify UI synchronization.

```python
class VoiceState(Enum):
    IDLE = 0          # Listening for wake word only
    WAKE_DETECTED = 1 # Wake word heard, preparing to listen
    LISTENING = 2     # Recording user speech
    TRANSCRIBING = 3  # Converting speech to text
    WAITING_RESPONSE = 4  # Waiting for OpenClaw response
    SPEAKING = 5      # Playing TTS output
```

**Key principles:**
- Components only emit events, never transition states directly
- State transitions are centralized in main orchestrator
- UI receives state change notifications via socket
- Supports barge-in: wake word during SPEAKING → cancel TTS → transition to LISTENING

## Components

### 1. Voice Engine (Python) - `voice/`

**Dependencies:**
- `openwakeword` - Wake word detection (80ms frames, 0-1 confidence)
- `mlx-whisper` or `lightning-whisper-mlx` - Speech-to-text on Apple Silicon
- `silero-vad` or `webrtcvad` - Voice activity detection for utterance boundaries
- `piper-tts` - Neural text-to-speech (opt-in for higher quality)
- `websockets` - Connect to OpenClaw Gateway
- `pyaudio` or `sounddevice` - Audio capture/playback
- `numpy` - Audio buffer handling

**Files:**
```
voice/
├── src/
│   ├── __init__.py
│   ├── main.py              # Entry point, orchestrates components
│   ├── state.py             # VoiceState enum and state machine
│   ├── audio.py             # Audio capture (microphone) and playback
│   ├── vad.py               # Voice activity detection (Silero/WebRTC)
│   ├── wake_word.py         # OpenWakeWord wrapper
│   ├── stt.py               # mlx-whisper speech-to-text
│   ├── tts.py               # TTS with AVSpeechSynthesizer (default) + Piper (opt-in)
│   ├── gateway_client.py    # OpenClaw WebSocket client (streaming-ready interface)
│   └── config.py            # Configuration (wake word, model paths, etc.)
├── scripts/
│   └── probe_gateway.py     # Minimal script to test Gateway protocol
├── slice0.py                # Vertical slice: ugly but working end-to-end demo
├── requirements.txt
└── pyproject.toml
```

**Flow:**
1. Audio stream continuously feeds OpenWakeWord (80ms chunks)
2. On wake word detection (score > threshold), transition to LISTENING state
3. Record until end-of-utterance detected via Silero VAD (not just amplitude)
4. Transcribe with mlx-whisper, send to OpenClaw via WebSocket
5. Receive streamed response chunks, speak via TTS
6. Return to IDLE state (wake word listening)

**Barge-in support:** Wake word detection continues during SPEAKING state. If detected, cancel TTS playback and transition to LISTENING.

### 2. macOS Overlay UI (SwiftUI) - `ui/JarvisOverlay/`

**Features:**
- Floating, borderless, always-on-top window
- Waveform visualization of audio input
- Status states: idle, listening, processing, speaking
- Click-through when idle (doesn't steal focus)
- Draggable to reposition

**Files:**
```
ui/JarvisOverlay/
├── JarvisOverlay/
│   ├── JarvisOverlayApp.swift
│   ├── ContentView.swift
│   ├── WaveformView.swift
│   ├── VoiceEngine.swift      # Communicates with Python voice engine
│   └── AudioVisualization.swift
├── JarvisOverlay.xcodeproj/
└── Package.swift
```

**Implementation notes:**
- Use `NSPanel` for always-on-top floating window
- Set `ignoresMouseEvents = true` when IDLE (click-through)
- Toggle `ignoresMouseEvents = false` when active (allow interaction)
- Draggable via custom drag gesture on panel

**Communication with Voice Engine:**
- Unix domain socket at `/tmp/openclaw_voice.sock` (cleaner than stdin/stdout, no buffering issues)
- JSON lines protocol for messages
- Events: `state_change`, `transcription`, `response_chunk`, `waveform_data`

**Waveform visualization:**
- Don't stream raw audio at 60fps
- Voice engine sends RMS/FFT buckets at ~20Hz
- UI interpolates for smooth animation

### 3. Model Downloads - `scripts/`

```
scripts/
├── download_models.sh       # Downloads all required models
└── setup.sh                 # Full setup script
```

**Models to download:**
- Wake word: `hey_jarvis.onnx` or custom model from OpenWakeWord
- Whisper: `mlx-community/whisper-base` (or tiny for 8GB machines, small/medium for 16GB+)
- Piper: `en_US-lessac-medium.onnx` (or other voice)

## Implementation Strategy: Vertical Slice

Instead of building complete layers then integrating, we build a minimal end-to-end slice first, then iterate. This proves the full loop works from day 1 and surfaces integration issues early.

```
Slice 0 (Ugly but working)     →  Slice 1 (Real components)  →  Slice 2 (Polish)
[Enter key → record → whisper     [Wake word → VAD → whisper    [UI → barge-in
 → gateway → say command]          → gateway → system TTS]       → streaming]
```

## Implementation Tasks

### Slice 0: Prove the Loop (1-2 hours)
Goal: Ugly end-to-end flow that proves every integration point works.

0. [ ] Set up Python project with minimal deps (websockets, sounddevice, mlx-whisper)
1. [ ] Probe Gateway protocol - discover session handling, message format
2. [ ] Build `slice0.py` - single file, no abstractions:
   ```python
   # Simulate wake word: press Enter to start
   # Record fixed 5 seconds of audio
   # Transcribe with mlx-whisper
   # Send to Gateway, print response
   # Speak with `say` command (subprocess)
   # Print state transitions to terminal
   ```
3. [ ] Verify: Can talk to OpenClaw and hear a response?

**Exit criteria:** You can press Enter, speak, and hear OpenClaw's response spoken aloud.

### Slice 1: Real Components (core work)
Goal: Replace fake/stub pieces with real implementations, one at a time.

4. [ ] Replace Enter key with OpenWakeWord detection
5. [ ] Replace fixed recording with Silero VAD end-of-utterance
6. [ ] Replace `say` with AVSpeechSynthesizer (for cancellation support)
7. [ ] Add VoiceState enum, refactor into state machine
8. [ ] Add Unix socket server for UI communication
9. [ ] Add minimal SwiftUI overlay (NSPanel + state indicator only)
10. [ ] Wire UI to voice engine via socket

**Exit criteria:** Wake word triggers recording, VAD ends it, response is spoken, UI shows state.

### Slice 2: Polish & Features
Goal: Production-ready features and UX improvements.

11. [ ] Add barge-in support (wake word cancels TTS)
12. [ ] Add waveform visualization (RMS/FFT at 20Hz)
13. [ ] Add Whisper warm-loading (keep loaded for N seconds)
14. [ ] Add Piper TTS as opt-in neural voice
15. [ ] Add click-through when idle, drag-to-reposition
16. [ ] Configuration file (config.yaml)
17. [ ] Model download scripts
18. [ ] Setup script and README

**Exit criteria:** Fully functional voice assistant with polished UI.

## OpenClaw Gateway Protocol (Research Notes)

Based on research, the Gateway uses:
- WebSocket at `ws://127.0.0.1:18789`
- Methods likely include:
  - `sessions_send` - Send a user message
  - `sessions_list` - List sessions
  - `sessions_history` - Get session history
  - Streaming response events

**Note:** Run `probe_gateway.py` early to discover session handling, message framing, and streaming format before building the full client.

## Design Notes

### Streaming TTS (Future-Ready)

V1 implementation: receive full response → speak

But design `gateway_client.py` interface to support streaming:
- Emit `response_chunk` events as they arrive
- TTS interface accepts chunks (even if it buffers internally for v1)

This enables incremental speech later (much lower perceived latency).

## Configuration

```yaml
# config.yaml
wake_word:
  model: "hey_jarvis"  # or path to custom model
  threshold: 0.5

stt:
  model: "mlx-community/whisper-base"  # Use base/tiny for 8GB machines
  language: "en"
  keep_warm_seconds: 30  # Keep model loaded after transcription to avoid cold-start

tts:
  engine: "system"  # "system" (AVSpeechSynthesizer) or "piper"
  system_voice: "com.apple.voice.compact.en-US.Samantha"
  piper_model: "en_US-lessac-medium"  # Only used if engine: "piper"
  rate: 1.0

gateway:
  url: "ws://127.0.0.1:18789"
  session_id: null  # auto-create or specify

audio:
  sample_rate: 16000
  chunk_size: 1280  # 80ms at 16kHz

vad:
  engine: "silero"  # "silero" (recommended) or "webrtc"
  threshold: 0.5
  min_silence_duration: 0.8  # seconds of silence to end utterance
```

## Risks & Mitigations

1. **Gateway protocol unknown** - May need to reverse-engineer from OpenClaw source or existing clients. Mitigation: Start with a simple test client to probe the protocol.

2. **Piper macOS ARM64 support** - Piper claims ARM64 support but may have issues. Mitigation: Fall back to `say` command or macOS AVSpeechSynthesizer if needed.

3. **Audio latency** - Multiple processing stages could add latency. Mitigation: Use streaming where possible, optimize buffer sizes.

4. **Model memory** - Whisper + wake word + TTS models in memory. Mitigation:
   - Only wake word model (~50MB) stays resident (required for continuous listening)
   - Lazy-load Whisper on wake detection, keep warm for N seconds, then unload
     - Avoids cold-start penalty if user speaks twice quickly
   - Use `AVSpeechSynthesizer` as default TTS (zero memory overhead)
   - Piper TTS available as opt-in "neural voice" mode
   - Use `whisper-base` (~150MB) or `whisper-tiny` (~75MB) for 8GB machines
