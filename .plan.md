# OpenClaw Voice Layer - Implementation Plan

## Overview
A local voice interface for OpenClaw that provides always-on wake word detection, speech-to-text, and text-to-speech with a minimal JARVIS-style overlay UI.

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     macOS Overlay (SwiftUI)                 │
│  - Floating waveform visualization                          │
│  - Status indicators (listening/processing/speaking)        │
│  - Minimal, always-on-top window                            │
└─────────────────────────────────────────────────────────────┘
                              │
                              │ Unix Socket / stdin-stdout
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    Voice Engine (Python)                     │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │ OpenWakeWord│→ │ mlx-whisper │→ │ OpenClaw WS Client  │  │
│  │ (wake word) │  │   (STT)     │  │ (send/receive msgs) │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
│                                              │               │
│                                              ▼               │
│                                    ┌─────────────────────┐  │
│                                    │ Piper TTS (speak)   │  │
│                                    └─────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │
                              │ WebSocket (ws://127.0.0.1:18789)
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                OpenClaw Gateway (existing)                   │
│  - sessions_send: send user message                         │
│  - streaming response events                                │
└─────────────────────────────────────────────────────────────┘
```

## State Machine

The voice engine uses an explicit state machine to prevent race conditions and simplify UI synchronization.

```python
class VoiceState(Enum):
    IDLE = 0          # Listening for wake word only
    WAKE_DETECTED = 1 # Wake word heard, preparing to listen
    LISTENING = 2     # Recording user speech
    TRANSCRIBING = 3  # Converting speech to text
    WAITING_RESPONSE = 4  # Waiting for OpenClaw response
    SPEAKING = 5      # Playing TTS output
```

**Key principles:**
- Components only emit events, never transition states directly
- State transitions are centralized in main orchestrator
- UI receives state change notifications via socket
- Supports barge-in: wake word during SPEAKING → cancel TTS → transition to LISTENING

## Components

### 1. Voice Engine (Python) - `voice/`

**Dependencies:**
- `openwakeword` - Wake word detection (80ms frames, 0-1 confidence)
- `mlx-whisper` or `lightning-whisper-mlx` - Speech-to-text on Apple Silicon
- `silero-vad` or `webrtcvad` - Voice activity detection for utterance boundaries
- `piper-tts` - Neural text-to-speech (opt-in for higher quality)
- `websockets` - Connect to OpenClaw Gateway
- `pyaudio` or `sounddevice` - Audio capture/playback
- `numpy` - Audio buffer handling

**Files:**
```
voice/
├── src/
│   ├── __init__.py
│   ├── main.py              # Entry point, orchestrates components
│   ├── state.py             # VoiceState enum and state machine
│   ├── audio.py             # Audio capture (microphone) and playback
│   ├── vad.py               # Voice activity detection (Silero/WebRTC)
│   ├── wake_word.py         # OpenWakeWord wrapper
│   ├── stt.py               # mlx-whisper speech-to-text
│   ├── tts.py               # TTS with AVSpeechSynthesizer (default) + Piper (opt-in)
│   ├── gateway_client.py    # OpenClaw WebSocket client (streaming-ready interface)
│   └── config.py            # Configuration (wake word, model paths, etc.)
├── scripts/
│   └── probe_gateway.py     # Minimal script to test Gateway protocol
├── requirements.txt
└── pyproject.toml
```

**Flow:**
1. Audio stream continuously feeds OpenWakeWord (80ms chunks)
2. On wake word detection (score > threshold), transition to LISTENING state
3. Record until end-of-utterance detected via Silero VAD (not just amplitude)
4. Transcribe with mlx-whisper, send to OpenClaw via WebSocket
5. Receive streamed response chunks, speak via TTS
6. Return to IDLE state (wake word listening)

**Barge-in support:** Wake word detection continues during SPEAKING state. If detected, cancel TTS playback and transition to LISTENING.

### 2. macOS Overlay UI (SwiftUI) - `ui/JarvisOverlay/`

**Features:**
- Floating, borderless, always-on-top window
- Waveform visualization of audio input
- Status states: idle, listening, processing, speaking
- Click-through when idle (doesn't steal focus)
- Draggable to reposition

**Files:**
```
ui/JarvisOverlay/
├── JarvisOverlay/
│   ├── JarvisOverlayApp.swift
│   ├── ContentView.swift
│   ├── WaveformView.swift
│   ├── VoiceEngine.swift      # Communicates with Python voice engine
│   └── AudioVisualization.swift
├── JarvisOverlay.xcodeproj/
└── Package.swift
```

**Implementation notes:**
- Use `NSPanel` for always-on-top floating window
- Set `ignoresMouseEvents = true` when IDLE (click-through)
- Toggle `ignoresMouseEvents = false` when active (allow interaction)
- Draggable via custom drag gesture on panel

**Communication with Voice Engine:**
- Unix domain socket at `/tmp/openclaw_voice.sock` (cleaner than stdin/stdout, no buffering issues)
- JSON lines protocol for messages
- Events: `state_change`, `transcription`, `response_chunk`, `waveform_data`

**Waveform visualization:**
- Don't stream raw audio at 60fps
- Voice engine sends RMS/FFT buckets at ~20Hz
- UI interpolates for smooth animation

### 3. Model Downloads - `scripts/`

```
scripts/
├── download_models.sh       # Downloads all required models
└── setup.sh                 # Full setup script
```

**Models to download:**
- Wake word: `hey_jarvis.onnx` or custom model from OpenWakeWord
- Whisper: `mlx-community/whisper-base` (or tiny for 8GB machines, small/medium for 16GB+)
- Piper: `en_US-lessac-medium.onnx` (or other voice)

## Implementation Tasks

### Phase 0: Protocol Discovery (Do First!)
0. [ ] Write minimal gateway probe script to understand protocol
   ```python
   async with websockets.connect(url) as ws:
       await ws.send(json.dumps({"type": "sessions_send", ...}))
       async for msg in ws:
           print(msg)
   ```
   - Discover: session ID requirements, message framing, streaming format

### Phase 1: Voice Engine Core
1. [ ] Set up Python project with pyproject.toml
2. [ ] Implement VoiceState enum and state machine
3. [ ] Implement audio capture with sounddevice
4. [ ] Integrate OpenWakeWord for wake word detection
5. [ ] Implement VAD with Silero VAD (not just amplitude threshold)
6. [ ] Integrate mlx-whisper for transcription
7. [ ] Implement TTS (AVSpeechSynthesizer default, Piper opt-in)
8. [ ] Create OpenClaw Gateway WebSocket client (streaming-ready interface)
9. [ ] Wire up the full voice loop with state machine
10. [ ] Add barge-in support (wake word cancels TTS)

### Phase 2: macOS Overlay UI
11. [ ] Create SwiftUI app with NSPanel (always-on-top, click-through when idle)
12. [ ] Implement waveform visualization (from RMS/FFT data, not raw audio)
13. [ ] Add status indicator synced to VoiceState
14. [ ] Connect to voice engine via Unix socket (`/tmp/openclaw_voice.sock`)
15. [ ] Handle drag-to-reposition

### Phase 3: Integration & Polish
16. [ ] Model download scripts
17. [ ] Full setup script (venv, deps, models)
18. [ ] Configuration file for customization
19. [ ] README with usage instructions

## OpenClaw Gateway Protocol (Research Notes)

Based on research, the Gateway uses:
- WebSocket at `ws://127.0.0.1:18789`
- Methods likely include:
  - `sessions_send` - Send a user message
  - `sessions_list` - List sessions
  - `sessions_history` - Get session history
  - Streaming response events

**Note:** Run `probe_gateway.py` early to discover session handling, message framing, and streaming format before building the full client.

## Design Notes

### Streaming TTS (Future-Ready)

V1 implementation: receive full response → speak

But design `gateway_client.py` interface to support streaming:
- Emit `response_chunk` events as they arrive
- TTS interface accepts chunks (even if it buffers internally for v1)

This enables incremental speech later (much lower perceived latency).

## Configuration

```yaml
# config.yaml
wake_word:
  model: "hey_jarvis"  # or path to custom model
  threshold: 0.5

stt:
  model: "mlx-community/whisper-base"  # Use base/tiny for 8GB machines
  language: "en"
  keep_warm_seconds: 30  # Keep model loaded after transcription to avoid cold-start

tts:
  engine: "system"  # "system" (AVSpeechSynthesizer) or "piper"
  system_voice: "com.apple.voice.compact.en-US.Samantha"
  piper_model: "en_US-lessac-medium"  # Only used if engine: "piper"
  rate: 1.0

gateway:
  url: "ws://127.0.0.1:18789"
  session_id: null  # auto-create or specify

audio:
  sample_rate: 16000
  chunk_size: 1280  # 80ms at 16kHz

vad:
  engine: "silero"  # "silero" (recommended) or "webrtc"
  threshold: 0.5
  min_silence_duration: 0.8  # seconds of silence to end utterance
```

## Risks & Mitigations

1. **Gateway protocol unknown** - May need to reverse-engineer from OpenClaw source or existing clients. Mitigation: Start with a simple test client to probe the protocol.

2. **Piper macOS ARM64 support** - Piper claims ARM64 support but may have issues. Mitigation: Fall back to `say` command or macOS AVSpeechSynthesizer if needed.

3. **Audio latency** - Multiple processing stages could add latency. Mitigation: Use streaming where possible, optimize buffer sizes.

4. **Model memory** - Whisper + wake word + TTS models in memory. Mitigation:
   - Only wake word model (~50MB) stays resident (required for continuous listening)
   - Lazy-load Whisper on wake detection, keep warm for N seconds, then unload
     - Avoids cold-start penalty if user speaks twice quickly
   - Use `AVSpeechSynthesizer` as default TTS (zero memory overhead)
   - Piper TTS available as opt-in "neural voice" mode
   - Use `whisper-base` (~150MB) or `whisper-tiny` (~75MB) for 8GB machines
