# OpenClaw Voice Layer - Implementation Plan

## Overview
A local voice interface for OpenClaw that provides always-on wake word detection, speech-to-text, and text-to-speech with a minimal JARVIS-style overlay UI.

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     macOS Overlay (SwiftUI)                 │
│  - Floating top-right overlay                               │
│  - Live transcription text                                  │
│  - Border ring animation (thinking/responding)              │
│  - Minimal, always-on-top window                            │
└─────────────────────────────────────────────────────────────┘
                              │
│ Unix Socket (/tmp/openclaw_voice.sock)
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    Voice Engine (Python)                     │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │ OpenWakeWord│→ │ mlx-whisper │→ │ OpenClaw WS Client  │  │
│  │ (wake word) │  │   (STT)     │  │ (send/receive msgs) │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
│                                              │               │
│                                              ▼               │
│                                    ┌─────────────────────┐  │
│                                    │ Piper TTS (speak)   │  │
│                                    └─────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │
                              │ WebSocket (ws://127.0.0.1:18789)
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                OpenClaw Gateway (existing)                   │
│  - sessions_send: send user message                         │
│  - streaming response events                                │
└─────────────────────────────────────────────────────────────┘
```

## State Machine

The voice engine uses an explicit state machine to prevent race conditions and simplify UI synchronization.

```python
class VoiceState(Enum):
    IDLE = 0          # Listening for wake word only
    WAKE_DETECTED = 1 # Wake word heard, preparing to listen
    LISTENING = 2     # Recording user speech
    TRANSCRIBING = 3  # Converting speech to text
    RESPONDING = 4    # Waiting/streaming OpenClaw response
    SPEAKING = 5      # Playing TTS output
```

**Key principles:**
- Components only emit events, never transition states directly
- State transitions are centralized in main orchestrator
- UI receives state change notifications via socket
- Supports barge-in: wake word during SPEAKING → cancel TTS → transition to LISTENING

## Components

### 1. Voice Engine (Python) - `voice/`

**Dependencies:**
- `openwakeword` - Wake word detection (80ms frames, 0-1 confidence)
- `mlx-whisper` or `lightning-whisper-mlx` - Speech-to-text on Apple Silicon
- `silero-vad` or `webrtcvad` - Voice activity detection for utterance boundaries
- `piper-tts` - Neural text-to-speech (opt-in for higher quality)
- `websockets` - Connect to OpenClaw Gateway
- `pyaudio` or `sounddevice` - Audio capture/playback
- `numpy` - Audio buffer handling

**Files:**
```
voice/
├── src/
│   ├── __init__.py
│   ├── main.py              # Entry point, orchestrates components
│   ├── state.py             # VoiceState enum and state machine
│   ├── audio.py             # Audio capture (microphone) and playback
│   ├── vad.py               # Voice activity detection (Silero/WebRTC)
│   ├── wake_word.py         # OpenWakeWord wrapper
│   ├── stt.py               # mlx-whisper speech-to-text
│   ├── tts.py               # TTS with AVSpeechSynthesizer (default) + Piper (opt-in)
│   ├── gateway_client.py    # OpenClaw WebSocket client (streaming-ready interface)
│   └── config.py            # Configuration (wake word, model paths, etc.)
├── scripts/
│   └── probe_gateway.py     # Minimal script to test Gateway protocol
├── slice0.py                # Vertical slice: ugly but working end-to-end demo
├── tests/
│   ├── __init__.py
│   ├── conftest.py          # Pytest fixtures
│   ├── functional/          # Tests requiring real hardware/services
│   │   ├── __init__.py
│   │   ├── test_audio.py    # Mic capture verification
│   │   ├── test_gateway.py  # Gateway connection/messaging
│   │   ├── test_stt.py      # Whisper transcription
│   │   ├── test_tts.py      # Speech output
│   │   ├── test_wake_word.py # Wake word detection
│   │   ├── test_vad.py      # Voice activity detection
│   │   └── test_e2e.py      # Full loop tests
│   └── unit/                # Pure unit tests (no hardware)
│       ├── __init__.py
│       └── test_state.py    # State machine logic
├── cli.py                   # CLI tool for testing/triggering components
├── requirements.txt
└── pyproject.toml
```

**Flow:**
1. Audio stream continuously feeds OpenWakeWord (80ms chunks)
2. On wake word detection (score > threshold), transition to LISTENING state
3. Record until end-of-utterance detected via Silero VAD (not just amplitude)
4. Transcribe with mlx-whisper, send to OpenClaw via WebSocket
5. Receive streamed response chunks, speak via TTS
6. Return to IDLE state (wake word listening)

**Barge-in support:** Wake word detection continues during SPEAKING state. If detected, cancel TTS playback and transition to LISTENING.

### 2. macOS Overlay UI (SwiftUI) - `ui/JarvisOverlay/`

**Features:**
- Floating, borderless, always-on-top window (top-right by default)
- Live transcription text while user is speaking
- Animated border ring while AI is thinking/responding
- Optional waveform visualization of audio input
- Status states: idle, listening, transcribing, responding, speaking
- Click-through when idle (doesn't steal focus)
- Draggable to reposition

**Files:**
```
ui/JarvisOverlay/
├── JarvisOverlay/
│   ├── JarvisOverlayApp.swift
│   ├── ContentView.swift
│   ├── WaveformView.swift
│   ├── VoiceEngine.swift      # Communicates with Python voice engine
│   ├── AudioVisualization.swift
│   └── CLIHandler.swift       # Handles CLI arguments for testing
├── JarvisOverlay.xcodeproj/
└── Package.swift
```

**CLI Gates for Testing:**
The UI accepts command-line arguments for programmatic testing:
```bash
# Set state directly
JarvisOverlay --state idle
JarvisOverlay --state listening
JarvisOverlay --state processing
JarvisOverlay --state speaking

# Send mock waveform data
JarvisOverlay --mock-waveform

# Test mode: cycle through all states
JarvisOverlay --test-cycle

# Connect to mock socket server
JarvisOverlay --socket /tmp/mock_voice.sock
```

**Implementation notes:**
- Use `NSPanel` for always-on-top floating window
- Set `ignoresMouseEvents = true` when IDLE (click-through)
- Toggle `ignoresMouseEvents = false` when active (allow interaction)
- Draggable via custom drag gesture on panel

**Communication with Voice Engine:**
- Unix domain socket at `/tmp/openclaw_voice.sock` (engine runs independently; UI reconnects)
- JSON lines protocol for messages
- Events: `state_change`, `transcription`, `response_chunk`, `waveform_data`

**Waveform visualization:**
- Don't stream raw audio at 60fps
- Voice engine sends RMS/FFT buckets at ~20Hz
- UI interpolates for smooth animation

### 3. Model Downloads - `scripts/`

```
scripts/
├── download_models.sh       # Downloads all required models
└── setup.sh                 # Full setup script
```

**Models to download:**
- Wake word: `hey_jarvis.onnx` or custom model from OpenWakeWord
- Whisper: `mlx-community/whisper-base` (or tiny for 8GB machines, small/medium for 16GB+)
- Piper: `en_US-lessac-medium.onnx` (or other voice)

## Implementation Strategy: Vertical Slice

Instead of building complete layers then integrating, we build a minimal end-to-end slice first, then iterate. This proves the full loop works from day 1 and surfaces integration issues early.

```
Slice 0 (Ugly but working)     →  Slice 1 (Real components)  →  Slice 2 (Polish)
[Enter key → record → whisper     [Wake word → VAD → whisper    [UI → barge-in
 → gateway → say command]          → gateway → system TTS]       → streaming]
```

## Implementation Tasks

### Slice 0: Prove the Loop (1-2 hours)
Goal: Ugly end-to-end flow that proves every integration point works.

0. [x] Set up Python project with minimal deps (websockets, sounddevice, mlx-whisper)
1. [x] Probe Gateway protocol - discover session handling, message format
2. [x] Build `slice0.py` - single file, no abstractions:
   ```python
   # Simulate wake word: press Enter to start
   # Record fixed 5 seconds of audio
   # Transcribe with mlx-whisper
   # Send to Gateway, print response
   # Speak with `say` command (subprocess)
   # Print state transitions to terminal
   ```
3. [x] Verify: Can talk to OpenClaw and hear a response?

**Exit criteria:** You can press Enter, speak, and hear OpenClaw's response spoken aloud.

### Slice 1: Real Components (core work)
Goal: Replace fake/stub pieces with real implementations, one at a time.

4. [x] Replace Enter key with OpenWakeWord detection
5. [ ] Replace fixed recording with Silero VAD end-of-utterance
6. [ ] Replace `say` with AVSpeechSynthesizer (for cancellation support)
7. [ ] Add VoiceState enum, refactor into state machine
8. [ ] Add Unix socket server for UI communication
9. [ ] Add minimal SwiftUI overlay (top-right, text + ring animation)
10. [ ] Wire UI to voice engine via socket

**Exit criteria:** Wake word triggers recording, VAD ends it, response is spoken, UI shows state.

### Slice 2: Polish & Features
Goal: Production-ready features and UX improvements.

11. [ ] Add barge-in support (wake word cancels TTS)
12. [ ] Add waveform visualization (RMS/FFT at 20Hz)
13. [ ] Add Whisper warm-loading (keep loaded for N seconds)
14. [ ] Add Piper TTS as opt-in neural voice
15. [ ] Add click-through when idle, drag-to-reposition
16. [ ] Configuration file (config.yaml)
17. [ ] Model download scripts
18. [ ] Setup script and README

**Exit criteria:** Fully functional voice assistant with polished UI.

## OpenClaw Gateway Protocol

WebSocket at `ws://127.0.0.1:18789`

**Frame Format:**
```
Request:  {type:"req", id, method, params}
Response: {type:"res", id, ok, payload|error}
Event:    {type:"event", event, payload, seq?}
```

**Authentication Handshake:**
1. Gateway sends `connect.challenge` event with nonce
2. Client sends `connect` request with role/scopes
3. Gateway responds with `hello-ok`

```json
// Connect request
{
  "type": "req",
  "id": 1,
  "method": "connect",
  "params": {
    "role": "operator",
    "scopes": ["operator.read", "operator.write"],
    "clientType": "voice",
    "clientVersion": "0.1.0"
  }
}
```

**Key Methods:**
- `chat.send` - Send user message, returns `runId`
- `chat.history` - Get session history
- `sessions.list` - List sessions

```json
// chat.send request
{
  "type": "req",
  "id": 2,
  "method": "chat.send",
  "params": {
    "sessionKey": "voice",
    "message": "Hello",
    "idempotencyKey": "uuid",
    "timeoutMs": 30000
  }
}
```

**Streaming Events:**
- `agent` event with `phase: "start"` / `phase: "end"`
- Text delta events with `text` or `content` field

**Reference:** Protocol derived from [OpenClaw iOS/Android source](https://github.com/openclaw/openclaw)

## Design Notes

### Streaming TTS (Future-Ready)

V1 implementation: receive full response → speak

But design `gateway_client.py` interface to support streaming:
- Emit `response_chunk` events as they arrive
- TTS interface accepts chunks (even if it buffers internally for v1)

This enables incremental speech later (much lower perceived latency).

## Configuration

```yaml
# config.yaml
wake_word:
  model: "hey_jarvis"  # or path to custom model
  threshold: 0.5

stt:
  model: "mlx-community/whisper-base"  # Use base/tiny for 8GB machines
  language: "en"
  keep_warm_seconds: 30  # Keep model loaded after transcription to avoid cold-start

tts:
  engine: "system"  # "system" (AVSpeechSynthesizer) or "piper"
  system_voice: "com.apple.voice.compact.en-US.Samantha"
  piper_model: "en_US-lessac-medium"  # Only used if engine: "piper"
  rate: 1.0

gateway:
  url: "ws://127.0.0.1:18789"
  session_id: null  # auto-create or specify

audio:
  sample_rate: 16000
  chunk_size: 1280  # 80ms at 16kHz

vad:
  engine: "silero"  # "silero" (recommended) or "webrtc"
  threshold: 0.5
  min_silence_duration: 0.8  # seconds of silence to end utterance
```

## Functional Testing

Each component has functional tests that verify real hardware/service integration. Run with:
```bash
# Run all functional tests
pytest tests/functional/ -v

# Run specific component test
pytest tests/functional/test_audio.py -v

# Skip tests requiring specific hardware
pytest tests/functional/ -v -m "not requires_mic"
```

### Test Markers
- `@pytest.mark.requires_mic` - Needs microphone access
- `@pytest.mark.requires_gateway` - Needs OpenClaw Gateway running
- `@pytest.mark.requires_speaker` - Needs audio output
- `@pytest.mark.slow` - Tests that take >5 seconds

### CLI Testing Tool
The `cli.py` tool allows triggering any component for manual verification:
```bash
# Test audio capture
python cli.py audio --record 3 --output test.wav

# Test gateway connection
python cli.py gateway --probe
python cli.py gateway --send "Hello"

# Test STT
python cli.py stt --file test.wav
python cli.py stt --record 5  # Record and transcribe

# Test TTS
python cli.py tts --speak "Hello world"
python cli.py tts --speak "Hello" --engine piper

# Test wake word
python cli.py wake-word --listen 30  # Listen for 30 seconds

# Test VAD
python cli.py vad --record  # Record until silence detected

# Test full loop (without UI)
python cli.py loop --headless

# Send mock events to UI
python cli.py ui --state listening
python cli.py ui --waveform-demo
```

## Risks & Mitigations

1. **Gateway protocol unknown** - May need to reverse-engineer from OpenClaw source or existing clients. Mitigation: Start with a simple test client to probe the protocol.

2. **Piper macOS ARM64 support** - Piper claims ARM64 support but may have issues. Mitigation: Fall back to `say` command or macOS AVSpeechSynthesizer if needed.

3. **Audio latency** - Multiple processing stages could add latency. Mitigation: Use streaming where possible, optimize buffer sizes.

4. **Model memory** - Whisper + wake word + TTS models in memory. Mitigation:
   - Only wake word model (~50MB) stays resident (required for continuous listening)
   - Lazy-load Whisper on wake detection, keep warm for N seconds, then unload
     - Avoids cold-start penalty if user speaks twice quickly
   - Use `AVSpeechSynthesizer` as default TTS (zero memory overhead)
   - Piper TTS available as opt-in "neural voice" mode
   - Use `whisper-base` (~150MB) or `whisper-tiny` (~75MB) for 8GB machines
